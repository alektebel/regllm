#!/bin/bash
# launch_ollama.sh â€” Start RegLLM with Ollama backend + Cloudflare public tunnel
#
# First time: build the model with ./scripts/build_ollama_model.sh
#
# Usage:
#   ./launch_ollama.sh               # port 7860, streaming UI
#   ./launch_ollama.sh --port 8080   # custom port

set -e
cd "$(dirname "$0")"

PORT=7860

for arg in "$@"; do
  case $arg in
    --port=*) PORT="${arg#*=}" ;;
    --port)   shift; PORT="$1" ;;
  esac
done

# â”€â”€â”€ Verify GGUF exists â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
GGUF="models/gguf/regllm-q4_k_m.gguf"
if [ ! -f "$GGUF" ]; then
  echo ""
  echo "âŒ  GGUF model not found: $GGUF"
  echo ""
  echo "   Build it first:"
  echo "   ./scripts/build_ollama_model.sh"
  echo ""
  exit 1
fi

# â”€â”€â”€ Verify Ollama model is registered â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if ! ollama list 2>/dev/null | grep -q "^regllm"; then
  echo ""
  echo "âš ï¸   regllm not found in Ollama â€” registering now..."
  # Need serve running to register
  ollama serve >/tmp/ollama_tmp.log 2>&1 &
  TEMP_SERVE=$!
  sleep 3
  ollama create regllm -f Modelfile
  kill "$TEMP_SERVE" 2>/dev/null || true
  sleep 1
  echo "âœ“  regllm registered"
fi

echo ""
echo "ðŸš€  Starting RegLLM + Ollama..."
echo "    Model  : regllm (Q4_K_M)"
echo "    Local  : http://localhost:$PORT"
echo ""

# â”€â”€â”€ Activate venv â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
source regllm.env/bin/activate

# â”€â”€â”€ Start Ollama server â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if curl -s http://localhost:11434 &>/dev/null; then
  echo "âœ“  Ollama already running"
  OLLAMA_PID=""
else
  echo "â–¶  Starting ollama serve..."
  ollama serve >/tmp/ollama.log 2>&1 &
  OLLAMA_PID=$!
  for i in $(seq 1 20); do
    if curl -s http://localhost:11434 &>/dev/null; then
      echo "âœ“  Ollama server up"
      break
    fi
    sleep 1
  done
fi

# â”€â”€â”€ Start Gradio â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
echo "â–¶  Starting Gradio UI..."
python app.py --backend ollama --port "$PORT" &
GRADIO_PID=$!

echo "â³  Waiting for Gradio (RAG init can take ~20 s)..."
for i in $(seq 1 60); do
  if curl -s "http://localhost:$PORT" &>/dev/null; then
    echo "âœ“  Gradio is up â†’ http://localhost:$PORT"
    break
  fi
  sleep 1
done

# â”€â”€â”€ Start Cloudflare tunnel â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
echo ""
echo "ðŸŒ  Starting Cloudflare tunnel..."
echo "    Your public URL will appear below (*.trycloudflare.com)"
echo ""
~/bin/cloudflared tunnel --url "http://localhost:$PORT" &
TUNNEL_PID=$!

# â”€â”€â”€ Cleanup on exit â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cleanup() {
  echo ""
  echo "Shutting down..."
  kill "$GRADIO_PID" "$TUNNEL_PID" 2>/dev/null || true
  [ -n "$OLLAMA_PID" ] && kill "$OLLAMA_PID" 2>/dev/null || true
}
trap cleanup EXIT INT TERM

echo ""
echo "Press Ctrl+C to stop."
wait "$GRADIO_PID"
